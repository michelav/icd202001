{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"imgs/unifor.jpg\" width=\"150px\">\n",
    "\n",
    "# Política no Twitter\n",
    "\n",
    "Usamos redes sociais para estabelecer contatos, exposição profissional e expressar opiniões. Redes como [Facebook](https://www.facebook.com/) e [Twitter](https://twitter.com/) recebem diariamente milhões de postagens e comentários de seus usuários. Considerando esse cenário, _seria possível observar padrões de comportamento em mensagens postadas no Twitter?_\n",
    "\n",
    "Este Notebook analisa dados da conta de dois políticos brasileiros, [Lula](https://twitter.com/LulaOficial) e [Jair Bolsonaro](https://twitter.com/jairbolsonaro) na tentativa de identificar esses padrões. \n",
    "\n",
    "Trabalho como resultado de Projeto da disciplina de **Introdução a Ciência de Dados**.\n",
    "\n",
    "## Ambiente\n",
    "Utilizamos [Python](https://www.python.org/) no ambiente [Jupyter](https://jupyter.org/) para desenvolver os Notebooks.\n",
    "\n",
    "Para montar o ambiente de execução você precisará de uma versão do [Conda](https://sandbox.anaconda.com/) e opcionalmente o [Git](https://git-scm.com/). Você encontrará instrução de instalação desses produtos em seus próprios sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/michelav/projeto-twitter.git\n",
    "cd projeto-twitter\n",
    "source path_to_conda/activate\n",
    "conda env create -f environment.yml\n",
    "conda activate ptwitter\n",
    "jupyter-lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "\n",
    "O _dataset_ contém todos os tweets publicados pelos dois políticos até o fim de 2019. Esses tweets estão divididos em dois (um para cada político) arquivos texto no formato ```json```. Os arquivos foram extraídos por meio da API Web da plataforma e seguem o formato definido no [_hotsite_](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json) da API.\n",
    "\n",
    "## Análise Exploratória\n",
    "\n",
    "Para a análise exploratória utilizaremos a biblioteca Pandas. Ele permite que organizemos os dados em _frames_ que permitem consulta, agregação e processamento massivo de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "\n",
    "dfs = []\n",
    "profiles = []\n",
    "to_be_dropped = ['id', 'display_text_range', 'source', \n",
    "                 'in_reply_to_status_id', 'in_reply_to_status_id_str', \n",
    "                 'in_reply_to_user_id', 'in_reply_to_user_id_str', \n",
    "                 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
    "                 'contributors', 'is_quote_status', 'lang', 'extended_entities',\n",
    "                 'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink']\n",
    "\n",
    "\n",
    "# object_hook para processar as strings do json como datetime\n",
    "def process_tweets(dct):\n",
    "    # Chaves e sub-chaves usadas para decompor as informacoes de hashtags e mencoes \n",
    "    meta = {'hashtags': 'text', 'user_mentions': 'screen_name'}\n",
    "    if 'created_at' in dct:\n",
    "        try:\n",
    "            dct['created_at'] = dt.datetime.strptime(dct['created_at'],'%a %b %d %H:%M:%S %z %Y').replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        except ValueError:\n",
    "            dct['created_at'] = np.nan\n",
    "    \n",
    "    for k, sk in meta.items():\n",
    "        # Procura pelas chaves de Meta no dicionario que represanta o Json\n",
    "        if k in dct:\n",
    "            # Itera pela lista de dicionarios em cada chave encontrada(ver estrutura do json) e agrega as sub-chaves em uma lista\n",
    "            items = dct[k]\n",
    "            dct[k] = [ivalue for item in items for ikey, ivalue in item.items() if ikey == sk] if items else np.nan\n",
    "    return dct\n",
    "\n",
    "with os.scandir('dados') as lsit:\n",
    "    fit = (f for f in lsit if f.is_file())\n",
    "    for f in fit:\n",
    "        with open(f.path, mode='r') as fp:\n",
    "            profile = f.name.split('.')[0]\n",
    "            profiles.append(profile)\n",
    "            tweets = json.load(fp, object_hook=process_tweets)\n",
    "            df = pd.DataFrame(tweets)\n",
    "            df.drop(columns=to_be_dropped, inplace=True)\n",
    "            df['tweet_len'] = df['full_text'].str.len()\n",
    "            df['profile'] = profile\n",
    "            df = df.join(pd.json_normalize(df['entities']).drop(columns=['symbols', 'urls', 'media']))\n",
    "            dfs.append(df)\n",
    "\n",
    "tweets_df = pd.concat(dfs)\n",
    "cat_type = CategoricalDtype(categories=profiles , ordered=True)\n",
    "tweets_df['profile'] = tweets_df['profile'].astype(cat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Criando um índice baseado no perfil do usuário, data de criação e identificação do tuíte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mindex = pd.MultiIndex.from_frame(tweets_df[['profile', 'created_at', 'id_str']])\n",
    "tweets_df.set_index(['profile', 'created_at', 'id_str'], inplace=True)\n",
    "tweets_df.sort_index(inplace=True)\n",
    "mindex = tweets_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Utilizando a biblioteca spacy para processar o texto dos tuítes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pt_core_news_sm\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji\n",
    "\n",
    "# Regras para desconsiderar tags e mentions na contagem de palavras\n",
    "is_hashtag_getter = lambda token: len(token.text) > 1 and token.text.startswith('#')\n",
    "is_mention_getter = lambda token: len(token.text) > 1 and token.text.startswith('@')\n",
    "is_currency_getter = lambda token: token.text.lower() == 'r$'\n",
    "is_abrev_getter = lambda token: len(token.text) <= 2 and token.text.lower() in ['c/', 'p/', 'q']\n",
    "\n",
    "Token.set_extension(\"is_hashtag\", getter=is_hashtag_getter, force=True)\n",
    "Token.set_extension(\"is_mention\", getter=is_mention_getter, force=True)\n",
    "Token.set_extension(\"is_currency\", getter=is_currency_getter, force=True)\n",
    "Token.set_extension(\"is_abrev\", getter=is_abrev_getter, force=True)\n",
    "\n",
    "# Novas stop words para PT \n",
    "custom_stop_words = ['a', 'e', 'o', 'n', 'd', 'A', 'E', 'O', 'N', 'D']\n",
    "\n",
    "nlp = pt_core_news_sm.load()\n",
    "\n",
    "# Configurando para remover emojis\n",
    "emoji = Emoji(nlp, merge_spans=False)\n",
    "nlp.add_pipe(emoji, first=True)\n",
    "\n",
    "# Configurando o modelo com as novas stop words\n",
    "for sw in custom_stop_words:\n",
    "    nlp.vocab[sw].is_stop = True\n",
    "\n",
    "# Configurando prefixos para n separar tags e @\n",
    "prefixes = list(nlp.Defaults.prefixes)\n",
    "# prefixes\n",
    "prefixes.remove(\"#\")\n",
    "# prefixes.append('R\\\\$')\n",
    "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search\n",
    "\n",
    "docs = list(nlp.pipe(tweets_df['full_text'].str.replace(r'\\n', '').to_numpy(),\n",
    "                     disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    return {'vocab_len': len(doc), 'palavras': [token.text.lower() for token in doc if  not token.is_stop \n",
    "         and not token.is_punct and not token.like_url and not token._.is_hashtag \n",
    "         and not token._.is_mention and not token._.is_currency and not token._.is_abrev\n",
    "         and not token._.is_emoji]}\n",
    "\n",
    "tweets_df = tweets_df.join(pd.DataFrame(list(map(process_doc, docs)), index=mindex, columns=['vocab_len', 'palavras']))\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de tweets por mês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.groupby(['profile', 'created_at'])['full_text'].count()\n",
    "# years = tweets_df.index.levels[1].year\n",
    "# months = tweets_df.index.levels[1].month\n",
    "di = pd.Grouper(freq='M', level='created_at')\n",
    "pi = pd.Grouper(level='profile')\n",
    "\n",
    "tweets_df.groupby([pi, di])['full_text'].count().head(50)\n",
    "\n",
    "# dir(g)\n",
    "# g.groups\n",
    "# g.get_group('jairbolsonaro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tweets_df.loc['jairbolsonaro', 'palavras'].explode()\n",
    "s.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels, data = zip(*words_freq.most_common(10))\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Frequência')\n",
    "ax.set_title('Palavras mais utilizadas pelo Lula')\n",
    "\n",
    "plt.bar(x, list(data))\n",
    "plt.xticks(x, labels, rotation=80)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "\n",
    "b = 'Cheio' if a else 'Vazio'\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(3, 8), index=['A', 'B', 'C'], columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(('olá', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
